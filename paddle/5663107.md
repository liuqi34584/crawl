## 任务描述



### 本次实践使用Python来爬取百度百科中《隐秘而伟大》所有演员的信息，以及收视率，并进行可视化分析。

### 数据获取：https://baike.baidu.com/item/隐秘而伟大

<br/>
<br/>


<img src ="https://ai-studio-static-online.cdn.bcebos.com/88e66d787b694dfb9e9a251dbbdcc4fe98679d6e829546dbbb3a749c7fd87517" height='500' width='500'/>
<img src="https://ai-studio-static-online.cdn.bcebos.com/8c681adcee5e450ca6998330b221feebcbdb2ed391db4ab1a34daec0f364a6d7" height='500' width='500' />



<br/>
<br/>


<br/>

**上网的全过程:**

    普通用户:

    打开浏览器 --> 往目标站点发送请求 --> 接收响应数据 --> 渲染到页面上。

    爬虫程序:

    模拟浏览器 --> 往目标站点发送请求 --> 接收响应数据 --> 提取有用的数据 --> 保存到本地/数据库。


**爬虫的过程**：

    1.发送请求（requests模块）

    2.获取响应数据（服务器返回）

    3.解析并提取数据（BeautifulSoup查找或者re正则）

    4.保存数据




<br/>

**本实践中将会使用以下两个模块，首先对这两个模块简单了解以下：**

<br/>

**request模块：**

    requests是python实现的简单易用的HTTP库，官网地址：http://cn.python-requests.org/zh_CN/latest/
    
    requests.get(url)可以发送一个http get请求，返回服务器响应内容。
    
    





<br/>

**BeautifulSoup库：**

    BeautifulSoup 是一个可以从HTML或XML文件中提取数据的Python库。网址：https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/
    
    BeautifulSoup支持Python标准库中的HTML解析器,还支持一些第三方的解析器,其中一个是 lxml。
    
    BeautifulSoup(markup, "html.parser")或者BeautifulSoup(markup, "lxml")，推荐使用lxml作为解析器,因为效率更高。


```python
# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:
# !mkdir /home/aistudio/external-libraries
# !pip install beautifulsoup4 -t /home/aistudio/external-libraries
# !pip install lxml -t /home/aistudio/external-libraries
```


```python
# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可:
import sys
sys.path.append('/home/aistudio/external-libraries')
```

## 数据爬取
## 一、爬取百度百科中《隐秘而伟大》中所有演员信息，返回页面数据


```python
import json
import re
import requests
import datetime
from bs4 import BeautifulSoup
import os


def crawl_wiki_data():
    """
    爬取百度百科中《隐秘而伟大》中演员信息，返回html
    """
    headers = { 
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
    }
    #url='https://baike.baidu.com/item/隐秘而伟大'  
    url = 'https://baike.baidu.com/item/%E9%9A%90%E7%A7%98%E8%80%8C%E4%BC%9F%E5%A4%A7/22454129?fr=aladdin'                       

    try:
        response = requests.get(url,headers=headers)
        #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串
        soup = BeautifulSoup(response.text,'lxml')           
        #返回class="lemmaWgt-roleIntroduction"的div,即“角色介绍”下方的div
        roleIntroductions = soup.find('div',{'class':'lemmaWgt-roleIntroduction'})
        all_roleIntroductions = roleIntroductions.find_all('li')
        print(all_roleIntroductions)
        actors = []
        for every_roleIntroduction in all_roleIntroductions:
             actor = {}    
             if every_roleIntroduction.find('div',{'class':'role-actor'}):
                 #找演员名称和演员百度百科连接     
                actor['role_name'] =  every_roleIntroduction.find('div',{'class':'role-name'}).text        
                actor["actor_name"] = every_roleIntroduction.find('div',{'class':'role-actor'}).find('a').text
                actor['actor_link'] =  'https://baike.baidu.com' + every_roleIntroduction.find('div',{'class':'role-actor'}).find('a').get('href')
                actor['role_escription'] = every_roleIntroduction.find('dd',{'class':'role-description'}).text   
             actors.append(actor)
    except Exception as e:
        print(e)
    json_data = json.loads(str(actors).replace("\'","\""))   
    with open('work/' + 'actors.json', 'w', encoding='UTF-8') as f:
        json.dump(json_data, f, ensure_ascii=False)


```

## 二、爬取每个演员的百度百科页面的信息，并进行保存


```python
def crawl_everyone_wiki_urls():
    '''
    爬取每个演员的百度百科图片，并保存
    ''' 
    with open('work/' + 'actors.json', 'r', encoding='UTF-8') as file:
         json_array = json.loads(file.read())
    headers = { 
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' 
     }  
    actor_infos = []
    for star in json_array:
        actor_info = {}       
        name = star['actor_name']
        link = star['actor_link']
        actor_info['name'] = name
        #向演员个人百度百科发送一个http get请求
        response = requests.get(link,headers=headers)        
        #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象
        bs = BeautifulSoup(response.text,'lxml')       
        #获取演员的民族、星座、血型、体重等信息
        base_info_div = bs.find('div',{'class':'basic-info cmn-clearfix'})
        dls = bs.find_all('dl')
        for dl in dls:
            dts = dl.find_all('dt')
            for dt in dts:
                if "".join(str(dt.text).split()) == '民族':
                     temp = dt.find_next('dd').text
                     actor_info['nation'] = "".join(temp.split())
                if "".join(str(dt.text).split()) == '星座':  
                     temp = dt.find_next('dd').text
                     actor_info['constellation'] = "".join(temp.split())
                if "".join(str(dt.text).split()) == '血型': 
                     temp = dt.find_next('dd').text
                     actor_info['blood_type'] = "".join(temp.split())
                if "".join(str(dt.text).split()) == '身高':  
                     height_str = str(dt.find_next('dd').text)
                     actor_info['height'] = str(height_str[0:height_str.rfind('cm')]).replace("\n","")
                if "".join(str(dt.text).split()) == '体重':  
                     temp = dt.find_next('dd').text
                     actor_info['weight'] = "".join(temp.split())
                if "".join(str(dt.text).split()) == '出生日期':  
                     birth_day_str = str(dt.find_next('dd').text).replace("\n","")
                     if '年' in  birth_day_str:
                         actor_info['birth_day'] = birth_day_str[0:birth_day_str.rfind('年')]
        actor_infos.append(actor_info) 
        #将演员个人信息存储到json文件中
        json_data = json.loads(str(actor_infos).replace("\'","\""))   
        with open('work/' + 'actor_infos.json', 'w', encoding='UTF-8') as f:
            json.dump(json_data, f, ensure_ascii=False)

     
```

## 三、爬取《隐秘而伟大》的收视情况，并保存


```python
def crawl_viewing_data():
    """
    爬取百度百科中《隐秘而伟大》收视情况，返回html
    """
    headers = { 
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
    }
    #url='https://baike.baidu.com/item/隐秘而伟大'  
    url = 'https://baike.baidu.com/item/%E9%9A%90%E7%A7%98%E8%80%8C%E4%BC%9F%E5%A4%A7/22454129?fr=aladdin'                        

    try:
        response = requests.get(url,headers=headers)
        #将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串
        soup = BeautifulSoup(response.text,'lxml')     
          
        #返回所有的<table>所有标签
        tables = soup.find_all('table')
        crawl_table_title = "收视情况"
        for table in  tables:           
            #对当前节点前面的标签和字符串进行查找
            table_titles = table.find_previous('div')
            for title in table_titles:
                if(crawl_table_title in title):
                    return table       
    except Exception as e:
        print(e)


```


```python


def parse_viewing_data(viewing_table):
    """
    对《隐秘而伟大》的收视情况table进行解析，并保存
    """
    viewing_datas = []

    trs = viewing_table.find_all('tr')

    for i in range(len(trs)):
        if i < 2:
            continue
        viewing_data = {}
        tds = trs[i].find_all('td')
        print(trs[i])
        viewing_data["broadcastDate"]= tds[0].text
        viewing_data["csm59_rating"]= tds[1].text
        viewing_data["csm59_rating_share"]= tds[2].text
        viewing_data["csm59_ranking"]= tds[3].text
        viewing_data["csm_rating"]= tds[4].text
        viewing_data["csm_rating_share"]= tds[5].text
        viewing_data["csm_ranking"]= tds[6].text
        viewing_datas.append(viewing_data)
    #将个人信息存储到json文件中
    json_data = json.loads(str(viewing_datas).replace("\'","\""))   
    with open('work/' + 'viewing_infos.json', 'w', encoding='UTF-8') as f:
        json.dump(json_data, f, ensure_ascii=False)


```

## 四、数据爬取主程序


```python
if __name__ == '__main__':

     #爬取百度百科中《隐秘而伟大》中演员信息，返回html
     html = crawl_wiki_data()

     #爬取每个演员的信息,并保存
     crawl_everyone_wiki_urls()
     
     # #1、爬取百度百科中《隐秘而伟大》收视情况，返回html   2、对《隐秘而伟大》的收视情况table进行解析，并保存
     viewing_table = crawl_viewing_data()
     parse_viewing_data(viewing_table)
     
     print("所有信息爬取完成！")


# 数据分析


```python
# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:
!mkdir /home/aistudio/external-libraries
!pip install matplotlib -t /home/aistudio/external-libraries
!pip install wordcloud -t /home/aistudio/external-libraries
```


```python
# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可:
import sys
sys.path.append('/home/aistudio/external-libraries')
```


```python
# 下载中文字体
#!wget https://mydueros.cdn.bcebos.com/font/simhei.ttf
# # 将字体文件复制到matplotlib字体路径
# !cp /home/aistudio/work/simhei.ttf /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/
# # 创建系统字体文件路径
!mkdir .fonts
# 复制文件到该路径
!cp  /home/aistudio/work/simhei.ttf  .fonts/
```

## 一、 绘制CSM59城市网收视率变化趋势


```python

import matplotlib.pyplot as plt
import numpy as np 
import json
import matplotlib.font_manager as font_manager
import pandas as pd
#显示matplotlib生成的图形
%matplotlib inline


df = pd.read_json('work/viewing_infos.json',dtype = {'broadcastDate' : str})
#print(df)

broadcastDate_list = df['broadcastDate']
csm59_rating_list = df['csm59_rating']

plt.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体
plt.figure(figsize=(15,8))
plt.title("《隐秘而伟大》CSM59城市网收视率变化趋势",fontsize=20) 
plt.xlabel("播出日期",fontsize=20) 
plt.ylabel("收视率%",fontsize=20) 
plt.xticks(rotation=45,fontsize=20)
plt.yticks(fontsize=20)
plt.plot(broadcastDate_list,csm59_rating_list) 
plt.grid() 
plt.savefig('/home/aistudio/work/chart01.jpg')
plt.show()
```

## 二、 绘制CSM59城市网和CSM全国网收视率变化趋势，并进行对比


```python
import matplotlib.pyplot as plt
import numpy as np 
import json
import matplotlib.font_manager as font_manager
import pandas as pd
#显示matplotlib生成的图形
%matplotlib inline


df = pd.read_json('work/viewing_infos.json',dtype = {'broadcastDate' : str})
#print(df)

broadcastDate_list = df['broadcastDate']
csm59_rating_list = df['csm59_rating']
csm_rating_list = df['csm_rating']

plt.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体
plt.figure(figsize=(15,8))
plt.title("《隐秘而伟大》收视率变化趋势",fontsize=20) 
plt.xlabel("播出日期",fontsize=20) 
plt.ylabel("收视率%",fontsize=20) 
plt.xticks(rotation=45,fontsize=20)
plt.yticks(fontsize=20)
plt.plot(broadcastDate_list,csm59_rating_list,label = "CSM59城市网收视率") 
plt.plot(broadcastDate_list,csm_rating_list,label = "CSM全国网收视率") 
plt.legend()
plt.grid() 
plt.savefig('/home/aistudio/work/chart02.jpg')
plt.show()
```

## 三、 绘制词云图


```python
import jieba
import collections
import wordcloud
```


```python
def generate_wc(string_data):
    # 文本预处理
    pattern = re.compile(u'\t|\n|\.|-|:|;|\)|\(|\?|"')       # 定义正则表达式匹配模式
    string_data = re.sub(pattern, '', string_data)           # 将符合模式的字符去除
    # 文本分词
    seg_list_exact = jieba.cut(string_data, cut_all = False) # 精确模式分词

    object_list = []
    remove_words = []
    
    #读取停用词
    with open("work/停用词库.txt",'r',encoding='utf-8') as fp:
        for word in fp:
            remove_words.append(word.replace("\n",""))
    
    for word in seg_list_exact:                              # 循环读出每个分词
        if word not in remove_words:                         # 如果不在去除词库中
            object_list.append(word)                         # 分词追加到列表

    # 词频统计
    word_counts = collections.Counter(object_list)           # 对分词做词频统计
    word_counts_top20 = word_counts.most_common(20)          # 获取前20最高频的词
    print(word_counts_top20)
    # 词频展示
    wc = wordcloud.WordCloud(
        font_path='work/simhei.ttf',                         # 设置字体格式
        background_color="#000000",                          # 设置背景图
        max_words=150,                                       # 最多显示词数
        max_font_size=60,                                    # 字体最大值
        width=707,
        height=499
    )
    wc.generate_from_frequencies(word_counts)                # 从字典生成词云

    plt.imshow(wc)                                           # 显示词云
    plt.axis('off')                                          # 关闭坐标轴
    plt.savefig('/home/aistudio/work/wordcloud.jpg')
    plt.show()                                               # 显示图像
```


```python

review_df = pd.read_json('work/actors.json')

content_str = ""
for row in review_df.index:
    content = review_df.loc[row,'role_escription']
    content_str += content

generate_wc(content_str)
```
